{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grundlagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'conda (Python 3.13.2)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /Users/h_schnaitter/code/Tabelle-Fallstudie-2/conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import sparqlkernel\n",
    "import SPARQLWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Abschnitt widmen wir uns einer praktischen SPARQL-Übung, indem wir uns mit der folgenden Forschungsfrage auseinandersetzen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wie viele Datensätze in der Form von offenen Daten bietet jedes Bundesland an?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst betrachten wir die technische Basis, mit der wir die SPARQL-Abfragen umsetzen werden. Wir arbeiten mit Jupyter Notebook - einem Dateiformat, welches ermöglicht, Erklärtext und Code in verschiedenen Programmiersprachen darzustellen. Dadurch wird die visuelle Darstellung von echtem Code für Abfragen und deren Ergebnisse ermöglicht. Zusätzlich können Erläuterungen zu den aufgerufenen Befehlen sowie weitere Kommentare dargestellt werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im nächsten Schritt können wir uns der Analyse der SPARQL-Syntax widmen, indem wir uns die Abfragen der Forschungsfragen ansehen. Als allererstes muss ein sogenannter \"endpoint\" definiert werden. Der endpoint ist die maschinelesbare Schnittstelle zum Repositorium, in dem jene Metadaten gespeichert sind, die wir abfragen wollen. Bei der Arbeit mit einem Online-SPARQL-Werkzeug ist die Definition eines endpoints häufig nicht nötig, da dieser schon automatisch definiert ist. Im Portal GOVDATA ist der endpoint die Schnittstelle zum deutschen Open Data Portal [govdata.de](https://www.govdata.de/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beispielhaft wollen wir nun das europäische Datenportal durchsuchen, welches wir auf der folgenden Webseite - finden. Anschließend verwenden wir folgende \"Magic\", d.h. einen Befehl, der Teil vom SPARQL-Python-Paket ist, der es uns erlaubt, alle künftigen Abfragen mit unserem endpoint zu verknüpfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%endpoint https://data.europa.eu/sparql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Mit dem festgelegten endpoint können wir SPARQL-Abfragen erstellen. Allerdings fragen wir nur jene Metadaten ab, welche zu den Datensätzen im Portal gespeichert sind. Siehe hierzu Kapitel ... . Bevor wir damit anfangen, möchten wir uns mit der Onthologie der gespeicherten Daten vertraut machen. Der im Kapitel DCAT_AP erklärte Metatenstandard DCAT-AP ist zentral für die Untersuchung der Metadateneigentschaften und definiert die Struktur und den Inhalt der Metadatenfelder - https://www.dcat-ap.de/def/. \n",
    "Schauen wir uns nun die Struktur unserer ersten Abfrage an."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SPARQL\n",
    "PREFIX dct: <http://purl.org/dc/terms/>\n",
    "PREFIX dcatde: <http://dcat-ap.de/def/dcatde/>\n",
    "PREFIX pg: <https://www.dcat-ap.de/def/politicalGeocoding/>\n",
    "\n",
    "SELECT ?uri ?title ?contributorid ?stateKey\n",
    "WHERE {\n",
    "    ?uri dct:title ?title .\n",
    "    ?uri dcatde:contributorID ?contributorid .\n",
    "  OPTIONAL {?uri pg:stateKey ?stateKey} .\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Erklärung des Codes\n",
    ":class: tip, dropdown\n",
    "\n",
    "Als erstes sehen wir die sogenannten PREFIXES. Die Prefixes sind ein nützliches Tool, das dabei hilft, auf diverse Eigenschaften zu verweisen und verkürzt Bezüge zwischen diesen Eigenschaften herzustellen. Sie sind zwar wichtig für eine einfachere Gestaltung der Abfragen, aber nicht essenziell. Sie helfen lediglich dabei, das Links nicht immer wieder ausgeschrieben werden müssen, und ermöglichen es nur die Endungen nach den Prefixes angeben zu müssen. Dies wird deutlicher, wenn wir uns den WHERE-Abschnitt ansehen.\n",
    "\n",
    "Vorerst betrachten wir aber den SELECT-Befehl. Mit SELECT wählen wir die Properties bzw. die Eigenschaften, die aufgelistet werden sollen. Jede Eigenschaft entspricht einer Spalte, die in der Tabelle mit Ergebnissen zu sehen ist. Da im SELECT-Befehl die folgenden 3 Properties ausgeschrieben werden - ?uri ?title ?contributorid ?stateKey - erhalten wir die URIs, die Titel, die Namen der Datenbereitsteller und das Kürzel des jeweiligen Landes, aus dem der Datensatz stammt.\n",
    "\n",
    "Die genauen Benennungen der Properties (Labels) werden im DCAT-AP-Handbuch definiert, auf das wir zurückgreifen müssen, um die genauen Labels für jede Property zu finden.\n",
    "\n",
    "Was Ihnen noch auffallen könnte ist, dass die Spalte für das Bundesland (stateKey) leer ist. Leider liegt das daran, dass das Land nicht codiert worden ist. Somit bleiben diese Felder leer. Dies ist ein klares Beispiel für lückenhaftes Metadatenmanagement, das die Beantowrtung unserer Forschungsfrage erschwert. In der Praxis kommt es oft zu Fällen, in denen Abfragen nicht sehr erfolgreich sind, wegen unvollständigen Metadatenbeschreibungen. \n",
    "\n",
    "Als nächstes haben wir den Kern jeder SPARQL-Abfrage - den WHERE-Befehl. Der WHERE-Befehl definiert die Beobachtungen, die aufgelistet werden sollen, indem die Bedingungen definiert werden. Somit werden nur die Beobachtungen aufgelistet, die alle Bedungungen erfüllen. In der Abfrage ist auch OPTIONAL zu sehen - dies besagt, dass die folgende Bedingung nicht zwingend zu erfüllen ist. Das bedeutet, dass selbst die Beobachtungen, in unserem Fall die Datensätze, in der Liste stehen, die keine Ausprägung für die Eigentschaft stateKey (Verweis auf Land) haben. Da leider stateKey nicht mit codiert ist, können wir uns alle Datensätze ansehen, die auf die anderen Bedingungen treffen (URI, Titel und ID der Bereitsteller), ohne dass wir eine leere Liste bekommen. OPTIONAL ist ein gutes Werkzeug, das benutzt werden kann, wenn man sich nicht sicher ist, ob jeweiligen Properties ordentlich codiert sind.\n",
    "\n",
    "Was wahrscheinlich noch auffällt ist, dass in jeder Zeile in der WHERE-Funktion 3 Elemente zu sehen sind. Diese Struktur ist essentiell für die SPARQL-Sprache - durch die sogenannten \"triplets\" werden Bezüge zwischen den Eigenschaften erstellt. Jede Zeile bestimmt einen Bezug zwischen 2 Eigenschaften. Die erste Eigenschaft ist somit das Subjekt (S), das zweite Element - der Bezug, der aus einem Prefix und einer zusätzlichen Spezifizierung besteht, heißt das Prädikat (P), und das dritte - die zweite Eigenschaft, ist das Objekt (O). P entspricht einem Link, der darauf verweist, wo die zweite Eigentschaft zu finden ist. Die Einordnung der Eigenschaften ist nach dem W3C-Standard, der schon *in einem früheren Kapitel erklärt wurde*, definiert. In dem DCAT-AP-Handbuch ist dann die genaue Verortung von jeder Eingenschaft zu finden. Durch die Triplets fragen wir genau ab, welche Datensätze wir erfragen wollen, je nach den Bedingungen, die solche Datensätze erfüllen sollen. Mit unserer Abfrage suchen wir die Datensätze ab, die einen Titel, ein URI, einen mitcodierten Datenbereitsteller, und wenn vorhanden, einen Schlüssel für das Bundesland, haben, was leider bei keinem der Datensätze der Fall ist. Es lässt sich darauf schließen, dass diese Felder nicht verpflichtend ausgefüllt werden müssen. \n",
    "\n",
    "Wichtig zu bedenken ist, dass SPARQL leider keine Paginierungsfunktion unterstützt. Man muss es in der Regel auf Anwendungsebene handhaben, da SPARQL von sich aus nicht das Durchblättern von Ergebnissen wie eine Weboberfläche unterstützt. Stattdessen muss die Paginierung manuell durch die Verwendung von LIMIT und OFFSET in den Abfragen implementiert werden. Dies erfordert eine zusätzliche Logik in der Anwendung, um die aktuelle Seite Ausgabe vollständig zu sehen. \n",
    "\n",
    "Leider konnten wir unsere Fragestellung wegen mangelhafter Daten nicht ganz beantworten. Deshalb versuchen wir, unsere Fragestellung zu ändern und zusätzliche Beispiele von SPARQL-Abfragen damit aufzuzeigen.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "PREFIX dct: <http://purl.org/dc/terms/>\n",
    "PREFIX dcatde: <http://dcat-ap.de/def/dcatde/>\n",
    "PREFIX pg: <https://www.dcat-ap.de/def/politicalGeocoding/>\n",
    "\n",
    "SELECT ?uri ?title ?contributorid ?stateKey\n",
    "WHERE {\n",
    "    ?uri dct:title ?title .\n",
    "    ?uri dcatde:contributorID ?contributorid .\n",
    "  OPTIONAL {?uri pg:stateKey ?stateKey} .\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
